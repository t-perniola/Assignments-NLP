% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "guidelines" option to generate the final version.
\usepackage[]{nlpreport} % show guidelines
%\usepackage[]{nlpreport} % hide guidelines


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for tables






% THE pdfinfo Title AND Author ARE NOT NECESSARY, THEY ARE METADATA FOR THE FINAL PDF FILE
\hypersetup{pdfinfo={
Title={Guidelines and template for NLP coursework report},
Author={Jane Smith \& John Doe}
}}
%\setcounter{secnumdepth}{0}  
 \begin{document}
%
\title{Assignment 2\\
\explanation{\rm Substitute the $\uparrow$ title $\uparrow$ with your project's title, or with Assignment 1 / 2\\ \smallskip}
% subtitle:
\large \explanation{\rm $\downarrow$ Keep only one of the following three  labels  / leave empty for assignments: $\downarrow$\\}
}
\author{Armando Renzullo, Cosimo Russo and Tommaso Perniola\\
Master's Degree in Artificial Intelligence, University of Bologna\\
\{ armando.renzullo, cosimo.russo, tommaso.perniola\}@studio.unibo.it
}
\maketitle

\attention{DO NOT MODIFY THIS TEMPLATE - EXCEPT, OF COURSE FOR TITLE, SUBTITLE AND AUTHORS.\\ IN THE FINAL VERSION, IN THE \LaTeX\ SOURCE REMOVE THE \texttt{guidelines} OPTION FROM  \texttt{$\backslash$usepackage[guidelines]\{nlpreport\}}.
}

\begin{abstract} This study evaluates in-context learning with two models, Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct, in zero-shot and few-shot scenarios, focusing on detecting sexist language. Results show that using multiple examples generally improves performance, but Mistral and Llama exhibit model-specific differences in accuracy and false positives. The findings highlight the need for tailored prompting strategies and diverse examples for optimal model performance.

%\begin{quote}
\explanation{
The abstract is very brief summary of your report. Try to keep it no longer than 15-20 lines at most. Write your objective, your approach, and your main observations (what are the findings that make this report worthwhile reading?)}
%\end{quote}

\end{abstract}

\attention{\textcolor{red}{NOTICE: THIS REPORT'S LENGTH MUST RESPECT THE FOLLOWING PAGE LIMITS: \begin{itemize}
    \item ASSIGNMENT: \textbf{2 PAGES} 
    \item NLP PROJECT OR PROJECT WORK: \textbf{8 PAGES}
    \item COMBINED NLP PROJECT + PW: \textbf{12 PAGES}
\end{itemize}  PLUS LINKS, REFERENCES AND APPENDICES.\\ 
THIS MEANS THAT YOU CANNOT FILL ALL SECTIONS TO MAXIMUM LENGTH. IT ALSO MEANS THAT, QUITE POSSIBLY, YOU WILL HAVE TO LEAVE OUT OF THE REPORT PART OF THE WORK YOU HAVE DONE OR OBSERVATIONS YOU HAVE. THIS IS NORMAL: THE REPORT SHOULD EMPHASIZE WHAT IS MOST SIGNIFICANT, NOTEWORTHY, AND REFER TO THE NOTEBOOK FOR ANYTHING ELSE.\\ 
FOR ANY OTHER ASPECT OF YOUR WORK THAT YOU WOULD LIKE TO EMPHASIZE BUT CANNOT EXPLAIN HERE FOR LACK OF SPACE, FEEL FREE TO ADD COMMENTS IN THE NOTEBOOK.\\ 
INTERESTING TEXT EXAMPLES THAT EXCEED THE MAXIMUM LENGTH OF THE REPORT CAN BE PLACED IN A DEDICATED APPENDIX AFTER THE REFERENCES.}}


\section{Introduction}
\label{sec:introduction}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 2 COLUMNS FOR PROJECT OR PW / 3 FOR COMBINED REPORTS.}

The rapid advancement of large language models (LLMs) has revolutionized natural language processing (NLP), offering unprecedented capabilities for tasks such as text generation, translation, and classification \citep{brown2020language, devlin2019bert}. Among these innovations, in-context learning (ICL) has emerged as a powerful paradigm, enabling models to perform specific tasks by providing examples directly within the input prompt \citep{dong2022survey}.

Traditional approaches to sexist language detection often rely on supervised learning, requiring large amounts of labeled data and fine-tuning, which can be resource-intensive. Moreover, biases in training data can perpetuate stereotypes or result in inaccuracies \citep{gpt3}. In contrast, ICL offers a more efficient method, where models adapt to specific tasks by leveraging examples within the input prompt without the need for extensive retraining \citep{min2022rethinking, zhao2021calibrate}. However, the effectiveness of ICL in detecting biased language remains underexplored, especially across different model architectures and prompt configurations.

This study investigates the effectiveness of ICL in detecting sexist language, focusing on two state-of-the-art LLMs: Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct. These models are selected due to their cutting-edge architecture and demonstrated success in a variety of NLP tasks. We evaluate their performance in both zero-shot and few-shot settings, comparing their ability to detect sexist language in a dataset of social media comments. Specifically, we analyze (1) the accuracy of detection, (2) the prevalence of false positives and false negatives, and (3) the impact of different prompting strategies.

\explanation{
The Introduction is an executive summary, which you can think of as an extended abstract.  Start by writing a brief description of the problem you are tackling and why it is important. (Skip it if this is an assignment report).} 

\explanation{Then give a short overview of known/standard/possible approaches to that problems, if any, and what are their advantages/limitations.} 

\explanation{After that, discuss your approach, and motivate why you follow that approach. If you are drawing inspiration from an existing model, study, paper, textbook example, challenge, \dots, be sure to add all the necessary references~\cite{DBLP:journals/corr/abs-2204-02311,DBLP:conf/acl/LorenzoMN22,DBLP:conf/clef/AnticiBIIGR21,DBLP:conf/ijcai/NakovCHAEBPSM21,DBLP:conf/naacl/RottgerVHP22,DBLP:journals/toit/LippiT16}.\footnote{\href{https://en.wikipedia.org/wiki/The_Muppet_Show}{Add only what is relevant.}}}

\explanation{Next, give a brief summary of your experimental setup: how many experiments did you run on which dataset. Last, make a list of the main results or take-home lessons from your work.}

\attention{HERE AND EVERYWHERE ELSE: ALWAYS KEEP IN MIND THAT, CRUCIALLY, WHATEVER TEXT/CODE/FIGURES/IDEAS/... YOU TAKE FROM ELSEWHERE MUST BE CLEARLY IDENTIFIED AND PROPERLY REFERENCED IN THE REPORT.}

\section{Background}
\label{sec:background}
\attention{MAX 2 COLUMNS (3 FOR COMBINED REPORTS). DO NOT INCLUDE SECTION IF NO BACKGROUND NECESSARY. OMIT SECTION IN ASSIGNMENT REPORTS.}

The detection of biased or harmful language, such as sexism, has become a significant challenge in natural language processing (NLP), particularly in domains like social media, customer support, and online forums, where large volumes of unstructured text need to be processed in real-time. Traditional methods for this task often rely on supervised learning, where models are trained on large, manually labeled datasets that include explicit examples of sexist language. However, acquiring such datasets can be costly and time-consuming, and the models themselves may inherit biases from the data or fail to generalize to new forms of biased language. In recent years, the advent of large pre-trained models, particularly those based on the Transformer architecture, has opened up new possibilities for addressing these challenges with less reliance on extensive labeled datasets.

\explanation{The Background section is where you briefly provide whatever background information on the domain or challenge you're addressing and/or on the techniques/approaches you're using, that (1) you think is necessary for the reader to understand your work and design choices, and (2) is not something that has been explained to you during the NLP course (to be clear: do NOT repeat explanations of things seen in class, we already know that stuff). If you adapt paragraphs from articles, books, online resources, etc: be sure to clarify which parts are yours and which ones aren't.}

\section{System description}
\label{sec:system}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 4 COLUMNS FOR PROJECT OR PW / 6 FOR COMBINED REPORTS.}

The proposed system utilizes pre-trained language models such as Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct for sexism detection in text, with a flexible pipeline that operates in both zero-shot and few-shot modes, depending on the availability of labeled data. The pipeline begins by formatting input texts into structured prompts that guide the model in the classification task, distinguishing between sexist and non-sexist language. In the zero-shot setting, the prompt contains only the task description, while in the few-shot setting, a small number of labeled examples are included to improve accuracy. Once the model generates a response, it is processed into a binary classification ("YES" or "NO"), which is then compared to the ground truth labels and evaluated using metrics such as accuracy and fail-ratio.

Mistral-7B-Instruct and Llama-3.1-8B-Instruct are both instruction-following models, but Mistral is typically smaller and more optimized for specific tasks, often providing faster inference for targeted applications. Llama, on the other hand, offers a more generalized performance across multiple languages and domains, making it more versatile for a variety of use cases.
\explanation{
Describe the system or systems you have implemented (architectures, pipelines, etc), and used to run your experiments. If you reuse parts of code written by others, be sure to make very clear your original contribution in terms of
\begin{itemize}
    \item architecture: is the architecture your design or did you take it from somewhere else
    \item coding: which parts of code are original or heavily adapted? adapted from existing sources? taken from external sources with minimal adaptations?
\end{itemize}
It is a good idea to add figures to illustrate your pipeline and/or architecture(s)
(see Figure~\ref{fig:architecture})\\
%
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{img/architecture.pdf}
    \caption{Model architecture}
    \label{fig:architecture}
\end{figure*}
}


\attention{MAX 2 COLUMNS / 3 FOR COMBINED REPORTS. OMIT SECTION IN ASSIGNMENT REPORTS.}

\explanation{Provide a brief description of your data including some statistics and pointers (references to articles/URLs) to be used to obtain the data. Describe any pre-processing work you did. Links to datasets must be placed later in Section~\ref{sec:links}.}



\section{Experimental setup and results}
\label{sec:results}
\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT OR PW / 5 FOR COMBINED REPORTS.}

In our experiments, we compared the performance of Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct in detecting sexism in text across different few-shot prompting configurations (0-shot, 2-shot, 3-shot, 4-shot). These configurations allowed us to evaluate model performance as the amount of labeled data increased.

We assessed performance using several metrics: precision, recall, F1-score, false positive rate (FPR), false negative rate (FNR), and fail-ratio. Precision and recall provided insight into the models’ ability to identify sexist and non-sexist language, while the F1-score balanced these metrics. The FPR and FNR indicated misclassification rates, and the fail-ratio gave an overall view of robustness.

Additionally, the quantization of the models plays a critical role in performance, influencing both the accuracy of results and the computational efficiency. Was used a 4-bit quantization to reduce memory usage and speed up inference, while maintaining model accuracy. Double quantization and NF4 format enhance precision, and using bfloat16 for computation ensures efficient processing with minimal loss in performance, making the model faster and more resource-efficient.

\explanation{
Describe how you set up your experiments: which architectures/configurations you used, which hyper-parameters and what methods used to set them, which optimizers, metrics, etc.
\\
Then, \textbf{use tables} to summarize your your findings (numerical results) in validation and test. If you don't have experience with tables in \LaTeX, you might want to use \href{https://www.tablesgenerator.com/}{\LaTeX table generator} to quickly create a table template.
}


\section{Discussion}
\label{sec:discussion}
\attention{MAX 1.5 COLUMNS FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT / 4 FOR COMBINED REPORTS. ADDITIONAL EXAMPLES COULD BE PLACED IN AN APPENDIX AFTER THE REFERENCES IF THEY DO NOT FIT HERE.}

Both Mistral and Llama initially struggle to accurately classify non-sexist texts, often mislabeling them as "sexist." For instance, at 0-shot, Mistral's recall is 0.98, while Llama's remains consistently above 0.84. As the number of examples increases, both models show improvement, with Mistral experiencing a more significant gain in precision—from 0.550 at 0-shot to 0.710 at 4-shot. Llama's precision increases more gradually, from 0.585 to 0.654 over the same range. However, despite these gains, Mistral starts to face challenges in correctly identifying "sexist" texts as the number of examples increases, with its recall dropping from 0.98 at 0-shot to 0.72 at 4-shot. In contrast, Llama's performance in identifying "sexist" texts remains stable. This trend suggests that Llama is the more stable and robust model overall, particularly evident in its steady improvement in the 4-shot configuration, where its F1-score increases from 0.692 at 0-shot to 0.738 at 4-shot. To further enhance performance, one potential improvement could be to adjust the prompt to provide a clearer definition of sexism within the context of the classification task.


\explanation{
Here you should make your analysis of the results you obtained in your experiments. Your discussion should be structured in two parts: 
\begin{itemize}
    \item discussion of quantitative results (based on the metrics you have identified earlier; compare with baselines);
    \item error analysis: show some examples of odd/wrong/unwanted  outputs; reason about why you are getting those results, elaborate on what could/should be changed in future developments of this work.
    
\end{itemize}
}



\section{Conclusion}
\label{sec:conclusion}
\attention{MAX 1 COLUMN.}

\explanation{
In one or two paragraphs, recap your work and main results.
What did you observe? 
Did all go according to expectations? 
Was there anything surprising or worthwhile mentioning?
After that, discuss the main limitations of the solution you have implemented, and indicate promising directions for future improvement.
}

The comparative analysis between Mistral and LLama highlights key differences in performance, stability, and sensitivity across various configurations. While LLama, as the larger model, demonstrates superior stability and adaptability due to its capacity for better generalization, Mistral shows a more pronounced improvement in the initial stages but struggles with recall and sensitivity as the number of examples increases. 

Both models initially face challenges in accurately classifying non-sexist texts as non-sexist, often misclassifying them as sexist. This suggests inherent biases potentially caused by imbalances in the training data or annotation processes. 

Future efforts to optimize these systems should focus on addressing these limitations through:
\begin{itemize}
    \item \textbf{Data Augmentation:} Enhancing the diversity of training data to improve the models' ability to generalize across different contexts.
    \item \textbf{Bias Mitigation:} Balancing training datasets to reduce the tendency to favor one class over the other.
    \item \textbf{Meta-Learning Techniques:} Incorporating advanced learning strategies to improve adaptability, particularly in detecting subtle patterns in complex datasets.
\end{itemize}

%\section{Links to external resources}
%\label{sec:links}
\attention{THIS SECTION IS OPTIONAL}
\explanation{
Insert here:
\begin{itemize}
    \item a link to your GitHub or any other public repo where one can find your code (only if you did not submit your code on Virtuale); 
    \item a link to your dataset (only for non-standard projects or project works).
\end{itemize}
}

\attention{DO NOT INSERT CODE IN THIS REPORT}




\bibliography{nlpreport.bib}
\end{document}