{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 1 </h1>\n",
    "<h3> 2. Load the three JSON files and encode them as pandas dataframes. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = '/Users/kor/Desktop/Unibo/II year/Fall/NLP/Project/Assignment 1/data/test.json'\n",
    "training_url = '/Users/kor/Desktop/Unibo/II year/Fall/NLP/Project/Assignment 1/data/training.json'\n",
    "validation_url = '/Users/kor/Desktop/Unibo/II year/Fall/NLP/Project/Assignment 1/data/validation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_json(training_url, orient='index')\n",
    "validation_set = pd.read_json(validation_url, orient='index')\n",
    "test_set = pd.read_json(test_url, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3. Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. <br>Items without a clear majority will be removed from the dataset. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(l):\n",
    "    y_count = l.count('YES')\n",
    "    n_count = l.count('NO')\n",
    "\n",
    "    if y_count == n_count:\n",
    "        return pd.NaT\n",
    "    \n",
    "    if y_count > 3:\n",
    "        return 'YES'\n",
    "\n",
    "    return 'NO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['hard_label_task1'] = training_set['labels_task1'].apply(majority)\n",
    "training_set.dropna(axis=0, inplace=True)\n",
    "\n",
    "validation_set['hard_label_task1'] = validation_set['labels_task1'].apply(majority)\n",
    "validation_set.dropna(axis=0, inplace=True)\n",
    "\n",
    "test_set['hard_label_task1'] = test_set['labels_task1'].apply(majority)\n",
    "test_set.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4. Filter the DataFrame to keep only rows where the `lang` column is `'en'`. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set[training_set['lang'] == lang]\n",
    "validation_set = validation_set[validation_set['lang'] == lang]\n",
    "test_set = test_set[test_set['lang'] == lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5. Remove unwanted columns: Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
    "validation_set = validation_set.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
    "test_set = test_set.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 6. Encode the `hard_label_task1` column: Use 1 to represent \"YES\" and 0 to represent \"NO\".</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['hard_label_task1'] = training_set['hard_label_task1'].map({'YES':1, 'NO':0})\n",
    "validation_set['hard_label_task1'] = validation_set['hard_label_task1'].map({'YES':1, 'NO':0})\n",
    "test_set['hard_label_task1'] = test_set['hard_label_task1'].map({'YES':1, 'NO':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hard_label_task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200006</th>\n",
       "      <td>200006</td>\n",
       "      <td>en</td>\n",
       "      <td>According to a customer I have plenty of time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200007</th>\n",
       "      <td>200007</td>\n",
       "      <td>en</td>\n",
       "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200008</th>\n",
       "      <td>200008</td>\n",
       "      <td>en</td>\n",
       "      <td>New to the shelves this week - looking forward...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203256</th>\n",
       "      <td>203256</td>\n",
       "      <td>en</td>\n",
       "      <td>idk why y’all bitches think having half your a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203257</th>\n",
       "      <td>203257</td>\n",
       "      <td>en</td>\n",
       "      <td>This has been a part of an experiment with @Wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203258</th>\n",
       "      <td>203258</td>\n",
       "      <td>en</td>\n",
       "      <td>\"Take me already\" \"Not yet. You gotta be ready...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203259</th>\n",
       "      <td>203259</td>\n",
       "      <td>en</td>\n",
       "      <td>@clintneedcoffee why do you look like a whore?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203260</th>\n",
       "      <td>203260</td>\n",
       "      <td>en</td>\n",
       "      <td>ik when mandy says “you look like a whore” i l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2870 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_EXIST lang                                              tweet  \\\n",
       "200002    200002   en  Writing a uni essay in my local pub with a cof...   \n",
       "200003    200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
       "200006    200006   en  According to a customer I have plenty of time ...   \n",
       "200007    200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
       "200008    200008   en  New to the shelves this week - looking forward...   \n",
       "...          ...  ...                                                ...   \n",
       "203256    203256   en  idk why y’all bitches think having half your a...   \n",
       "203257    203257   en  This has been a part of an experiment with @Wo...   \n",
       "203258    203258   en  \"Take me already\" \"Not yet. You gotta be ready...   \n",
       "203259    203259   en  @clintneedcoffee why do you look like a whore?...   \n",
       "203260    203260   en  ik when mandy says “you look like a whore” i l...   \n",
       "\n",
       "        hard_label_task1  \n",
       "200002                 1  \n",
       "200003                 1  \n",
       "200006                 1  \n",
       "200007                 1  \n",
       "200008                 0  \n",
       "...                  ...  \n",
       "203256                 1  \n",
       "203257                 1  \n",
       "203258                 1  \n",
       "203259                 1  \n",
       "203260                 1  \n",
       "\n",
       "[2870 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 2 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remove emojis** from the tweets.\n",
    "- **Remove hashtags** (e.g., `#example`).\n",
    "- **Remove mentions** such as `@user`.\n",
    "- **Remove URLs** from the tweets.\n",
    "- **Remove special characters and symbols**.\n",
    "- **Remove specific quote characters** (e.g., curly quotes).\n",
    "- **Perform lemmatization** to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_list = map(lambda x: ''.join(x.split()), emoji.EMOJI_DATA.keys())\n",
    "\n",
    "EMOJI_RE = re.compile('|'.join(re.escape(p) for p in emojis_list))\n",
    "HASHTAGS_RE = re.compile('#\\w+')\n",
    "MENTIONS_RE = re.compile('@\\w+')\n",
    "URL_RE = re.compile('(https|http)?:\\/\\/\\S+')\n",
    "#Qui ho aggiunto il punto sostituito dallo spazio perchè mi sembra che la maggior parte \n",
    "#dei tweets ne traggono beneficio, poi TODO va provato raga\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|,;‘’“”\\\"\\.]')\n",
    "SPECIAL_CHARACTERS_RE = re.compile('&amp;')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^\\w+ +]')\n",
    "\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    return EMOJI_RE.sub(' ',text)\n",
    "\n",
    "def remove_hashtags(text: str) -> str:\n",
    "    return HASHTAGS_RE.sub(' ', text)\n",
    "\n",
    "def remove_mentions(text: str) -> str:\n",
    "    return MENTIONS_RE.sub(' ', text)\n",
    "\n",
    "def remove_url(text: str) -> str:\n",
    "    return URL_RE.sub(' ',text)\n",
    "\n",
    "def remove_special_characters(text: str) -> str:\n",
    "    return SPECIAL_CHARACTERS_RE.sub('', text)\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some hashtags in the form \"#somethinghttps://\" also removes the initial part of the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@rufinelix's account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Dict\n",
    "from functools import reduce\n",
    "\n",
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          remove_emojis,\n",
    "                          remove_hashtags,\n",
    "                          remove_mentions,\n",
    "                          remove_url,\n",
    "                          remove_special_characters,\n",
    "                          replace_special_characters,\n",
    "                          filter_out_uncommon_symbols,\n",
    "                          remove_stopwords,\n",
    "                          strip_text\n",
    "                          ]\n",
    "#Lui elimina anche le stopwords, poi TODO va provato raga\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n",
      "\n",
      "[Debug] Before:\n",
      "According to a customer I have plenty of time to go spent the Stirling coins he wants to pay me with, in Derry. \"Just like any other woman, I'm sure of it.\" #EveryDaySexism in retail.\n",
      "\n",
      "[Debug] After:\n",
      "according customer plenty time go spent stirling coins wants pay derry like woman im sure retail\n",
      "\n",
      "Pre-processing completed!\n"
     ]
    }
   ],
   "source": [
    "print('Pre-processing text...')\n",
    "\n",
    "print()\n",
    "print(f'[Debug] Before:\\n{training_set.tweet.values[2]}')\n",
    "print()\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "training_set['tweet'] = training_set['tweet'].apply(lambda txt: text_prepare(txt))\n",
    "validation_set['tweet'] = validation_set['tweet'].apply(lambda txt: text_prepare(txt))\n",
    "test_set['tweet'] = test_set['tweet'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(f'[Debug] After:\\n{training_set.tweet.values[2]}')\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200002    writing uni essay local pub coffee random old ...\n",
       "200003    2021 1921 dont appreciate two rides team membe...\n",
       "200006    according customer plenty time go spent stirli...\n",
       "200007    blokes drink beer sorry arent bloke drink wine...\n",
       "200008       new shelves week looking forward reading books\n",
       "200010                      guess fairly normal neanderthal\n",
       "200011    means women usually end lower paid support wor...\n",
       "200013    hi orla interesting piece 2 policy response be...\n",
       "200015    dear god colette capable identifying sexism li...\n",
       "200016                            women home cooking family\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Focus Focus Focus qui le contrazioni tipo I'm le accorpa, ma sotto la lemmatization con wordNet le \n",
    "#scoppia. Se usassimo le stopwords le eliminerebbe direttamente e il problema non si porrebbe, quindi indovinate\n",
    "#TODO va provato raga\n",
    "training_set.iloc[:10]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "def get_wordnet_key(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return 'n'\n",
    "\n",
    "def lem_text(text: str):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200002    writing uni essay local pub coffee random old ...\n",
       "200003    2021 1921 dont appreciate two rides team membe...\n",
       "200006    according customer plenty time go spent stirli...\n",
       "200007    blokes drink beer sorry arent bloke drink wine...\n",
       "200008       new shelves week looking forward reading books\n",
       "200010                      guess fairly normal neanderthal\n",
       "200011    means women usually end lower paid support wor...\n",
       "200013    hi orla interesting piece 2 policy response be...\n",
       "200015    dear god colette capable identifying sexism li...\n",
       "200016                            women home cooking family\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['tweet'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_train_texts = [lem_text(text) for text in training_set['tweet']]\n",
    "lem_validation_texts = [lem_text(text) for text in validation_set['tweet']]\n",
    "lem_test_texts = [lem_text(text) for text in test_set['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['write uni essay local pub coffee random old man keep ask drunk question im try concentrate end good luck youll end get marry use anyway alive well',\n",
       " '2021 1921 dont appreciate two ride team member look behind ask man behind many party impress',\n",
       " 'accord customer plenty time go spent stirling coin want pay derry like woman im sure retail',\n",
       " 'bloke drink beer sorry arent bloke drink wine apparently alive well',\n",
       " 'new shelf week look forward read book',\n",
       " 'guess fairly normal neanderthal',\n",
       " 'mean woman usually end low pay support work start change traditionalist notice unfairness previously hasnt bother',\n",
       " 'hi orla interest piece 2 policy response believe earlyinlife prosecution punishment way go boy young men offend girl woman see zero tolerance approach institute',\n",
       " 'dear god colette capable identify sexism literally anywhere good see develop female grandpa simpson',\n",
       " 'woman home cooking family']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_train_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['tweet'] = lem_train_texts\n",
    "validation_set['tweet'] = lem_validation_texts\n",
    "test_set['tweet'] = lem_test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 3 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed words using **GloVe embeddings**. <br>\n",
    "You are **free** to pick any embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str], Dict[str, int], List[str]):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 0\n",
    "    for sentence in df.tweet:\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = curr_idx\n",
    "                idx_to_word[curr_idx] = token\n",
    "                curr_idx += 1\n",
    "    \n",
    "    word_to_idx['UNK'] = curr_idx\n",
    "    idx_to_word[curr_idx] = 'UNK'\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Index -> Word vocabulary size: 9362\n",
      "[Debug] Word -> Index vocabulary size: 9362\n",
      "[Debug] Some words: [('uni', 1), ('essay', 2), ('local', 3), ('pub', 4), ('coffee', 5), ('random', 6), ('old', 7), ('man', 8), ('keep', 9), ('ask', 10)]\n"
     ]
    }
   ],
   "source": [
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(training_set)\n",
    "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word)}')\n",
    "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx)}')\n",
    "print(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10) + 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "emb_model = gloader.load(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 1339 (14.30%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms = check_OOV_terms(emb_model, word_listing)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int],\n",
    "                           vocab_size: int,\n",
    "                           oov_terms: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in word_to_idx.items():\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (9362, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(emb_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TASK 4 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
